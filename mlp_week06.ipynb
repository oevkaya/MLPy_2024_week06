{"cells":[{"cell_type":"markdown","metadata":{"id":"6Og4DnJPrB4A","cell_id":"fe4ac837a50442aab0c798af200078d1","deepnote_cell_type":"markdown"},"source":"# Week 6 - Logistic Regression\n\n### Aims\n\nBy the end of this notebook you will be able to understand \n\n>* the binary logistic regression\n>* the main performance metrics for classification\n>* the improvement for the imbalanced data \n>* the basics of multi-class classification\n\n1. [Setup](#setup)\n\n2. [Binary Logistic Regression](#RBH)\n\n3. [Regularization Example](#SKV)\n\n4. [Imbalanced Data](#Imbal)\n\n5. [Multi-class Example](#mclog)\n","block_group":"00000-34f9557d-8f15-494e-8d65-74c8ae429c5c"},{"cell_type":"markdown","metadata":{"id":"AdHUSbWsvZ7h","cell_id":"f175a3fbdf0a4b65af031271d662692a","deepnote_cell_type":"markdown"},"source":"This week we will be implementing the logistic regression for a classification problem. \n\n- For this purpose, the main data set is called `Default.csv` in the upcoming code snippets.\n- For the multi-class example, we will turn back to the `iris data` example for the simplicity.\n\nDuring workshops, you will complete the worksheets together in teams of 2-3, using **pair programming**. You should aim to switch roles between driver and navigator approximately every 15 minutes. When completing worksheets:\n\n>- You will have tasks tagged by (CORE) and (EXTRA). For some code chunks. fill in the blanks with the suitable inputs, when you need!\n>- Your primary aim is to complete the (CORE) components during the WS session, afterwards you can try to complete the (EXTRA) tasks for your self-learning process. \n>- Look for the 🏁 as cue to switch roles between driver and navigator.\n>- In some Exercises, you will see some beneficial hints at the bottom of questions.\n\nInstructions for submitting your workshops can be found at the end of worksheet. As a reminder, you must submit a pdf of your notebook on Learn by 16:00 PM on the Friday of the week the workshop was given.","block_group":"1f52adc1113949ebadcf8abd7f0ce1d2"},{"cell_type":"markdown","metadata":{"id":"6sVlUI4SvZ7i","cell_id":"bd16b0edf2c0469fa259bbc88339125e","deepnote_cell_type":"markdown"},"source":"---\n\n# 1. General Setup <a id='setup'></a>\n\n## 1.1. Packages\n\nNow lets load in the packages you wil need for this workshop. Maybe there are some missing packages below or the ones with that is not necessary based on your","block_group":"00001-645a25eb-6010-425a-88c0-ecf0093a9edc"},{"cell_type":"code","metadata":{"id":"grVNp8GrrH0g","output_cleared":true,"cell_id":"2eac4e6378444f76bb884ae6d94a927c","deepnote_cell_type":"code"},"source":"# Display plots inline\n%matplotlib inline\n\n# Data libraries\nimport pandas as pd\nimport numpy as np\n\n# Plotting libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# sklearn modules\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import mean_squared_error\n\n# Other necessary packages\nfrom sklearn.datasets import load_iris           # for the Iris data\nfrom IPython.display import Image                # displaying .png images\nfrom sklearn.preprocessing import StandardScaler # scaling features\nfrom sklearn.preprocessing import LabelEncoder   # binary encoding\nfrom sklearn.pipeline import Pipeline            # combining classifier steps\nfrom sklearn.preprocessing import PolynomialFeatures # make PolynomialFeatures\nimport warnings # prevent warnings\nfrom time import time\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, KFold, StratifiedKFold #About randomized search over parameters\nfrom scipy.stats.distributions import uniform, loguniform # About creating random C values for regularization\n\n# from imblearn.under_sampling import RandomUnderSampler\nfrom collections import Counter\n# from imblearn.metrics import classification_report_imbalanced\n# import re\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix","block_group":"00002-d0af5d8f-8894-4c5a-b754-353993666790","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"cell_id":"712f36fd0bda4fc8b8db44dd8eeb411c","deepnote_cell_type":"code"},"source":"# Plotting defaults\nplt.rcParams['figure.figsize'] = (8,5)\nplt.rcParams['figure.dpi'] = 80","block_group":"9ed2325fb49449c394a7d05286cb25f0","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"N8_vjOPKdqLm","cell_id":"0b23ae4f5dbf4cd498a4347bce56c051","deepnote_cell_type":"markdown"},"source":"##  1.2 User Defined Helper Functions\n\nBelow are three helper functions we will be using in this workshop. You can create your own if you think it is necessary OR directly use already available helper functions within `sklearn library`.  \n\n- `tidy_scores()`: Simple function for getting the summary of cross-validation results in a tidy way.\n- `pretty_confusion_matrix()`: Returns the confusion matrix in a nicer way but works for only binary classification!\n- `plot_roc_curve()` : ROC curve plotting function based on a given threshold\n\nYou can modify the following functions based on your needs as well. These practices would be important while you are working on your project either. ","block_group":"ff714da526394a8583ef7cec4fee1fc4"},{"cell_type":"code","metadata":{"cell_id":"9a8af39c588648b3a849f2c77fcebf0e","deepnote_cell_type":"code"},"source":"# tidy the output into a dataframe\n# We will use for getting the summary of cross-validation results in some parts\ndef tidy_scores(score_dict):\n    df = pd.DataFrame(score_dict)\n    df.loc['mean'] = df.mean()\n    df.loc['sd'] = df.std()\n    df.rename({\"test_score\":\"val_score\"}, axis=1, inplace=True)\n    df.index.name = \"fold\"\n    return df.round(2)","block_group":"16f9bdeddacd4db6b319edf7a229487c","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"cell_id":"0d0022753d944b588325a8c663047c42","deepnote_cell_type":"code"},"source":"# this creates the matplotlib graph to make the confmat look nicer\n# WARNING 1: IT WORKS FOR ONLY BINARY CLASSIFICATION CASE !!!\n# WARNING 2: The locations of TP, FP, TN, and FN are varying when you compared with our notes !!!\n\ndef pretty_confusion_matrix(confmat, labels, title, labeling=False, highlight_indexes=[]):\n\n    labels_list = [[\"TN\", \"FP\"], [\"FN\", \"TP\"]]\n    \n    fig, ax = plt.subplots(figsize=(4, 4))\n    ax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\n    for i in range(confmat.shape[0]):\n        for j in range(confmat.shape[1]):\n            if labeling:\n                label = str(confmat[i, j])+\" (\"+labels_list[i][j]+\")\"\n            else:\n                label = confmat[i, j]\n            \n            \n            if [i,j] in highlight_indexes:\n                ax.text(x=j, y=i, s=label, va='center', ha='center',\n                        weight = \"bold\", fontsize=18, color='#32618b')\n            else:\n                ax.text(x=j, y=i, s=label, va='center', ha='center')\n       \n    # change the labels\n    with warnings.catch_warnings():\n        warnings.simplefilter(\"ignore\")\n        ax.set_xticklabels(['']+[labels[0], labels[1]])\n        ax.set_yticklabels(['']+[labels[0], labels[1]])\n\n    plt.xlabel('Predicted Label')\n    plt.ylabel('True Label')\n    ax.xaxis.set_label_position('top')\n    plt.suptitle(title)\n    plt.tight_layout()\n    \n    plt.show()","block_group":"003c961a5c2440c6885a8cb3e4cb815c","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"cell_id":"a751aa606aa1487ab8e28be091186efa","deepnote_cell_type":"code"},"source":"#ROC curve drawing by using the helper function\n\ndef plot_roc_curve(roc):\n    \"\"\"\n    Plots the ROC curve from a DataFrame.\n    \n    Parameters:\n    - roc: DataFrame containing 'false positive rate', 'true positive rate', and 'threshold' columns.\n    \"\"\"\n    plt.figure(figsize=(8, 6))\n    plt.plot(roc['false positive rate'], roc['true positive rate'], label='ROC Curve')\n    plt.plot([0, 1], [0, 1], 'k--', label='Random Guess')  # adding a 45-degree dashed line\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver Operating Characteristic (ROC) Curve')\n    plt.legend(loc='lower right')\n    plt.grid(True)\n    plt.show()","block_group":"5a64387fcda446ac91b394028fd9ed4f","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"yz3bjxcbvZ7r","cell_id":"801dbff511c947ada7eaa2b08ae74f88","deepnote_cell_type":"markdown"},"source":"## 1.3 Data\n\nThe dataset consists of **10000** individuals and whether their credit card has defaulted or not. \n\nBelow is the column description: The main aim is to build the model using Logistic Regression and predict the person who is defaulted or not (classifying each individual). The included columns in the data set are as follows:\n\n* `default` - Whether the individual has defaulted\n\n* `student` - Whether the individual is the student\n\n* `balance` - The balance in the individual's account\n\n* `income` - Income of an individual\n\nWe read the data into python using pandas.\n\n\n\n","block_group":"00003-90709695-8746-4669-9199-fd144a6ec872"},{"cell_type":"code","metadata":{"id":"eF19U6ivvZ7r","colab":{"height":206,"base_uri":"https://localhost:8080/"},"outputId":"fa3accef-409c-41d9-b61d-2ac6b5d8f4b6","cell_id":"464e9c37fabb411084268756132a9e7e","deepnote_cell_type":"code"},"source":"df_default = pd.read_csv(\"Default.csv\", index_col=0)\n\n# Lets just drop the student varible, \n# for simplicity we will focus on two main predictors; balance and income\ndf_default = df_default.drop(\"student\", axis=1)\ndf_default.head()","block_group":"c4f3027986ae4b458970dc21ac3b4567","execution_count":null,"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>default</th>\n      <th>balance</th>\n      <th>income</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>1</th>\n      <td>No</td>\n      <td>729.526495</td>\n      <td>44361.62507</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>No</td>\n      <td>817.180407</td>\n      <td>12106.13470</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>No</td>\n      <td>1073.549164</td>\n      <td>31767.13895</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>No</td>\n      <td>529.250605</td>\n      <td>35704.49394</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>No</td>\n      <td>785.655883</td>\n      <td>38463.49588</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"  default      balance       income\n1      No   729.526495  44361.62507\n2      No   817.180407  12106.13470\n3      No  1073.549164  31767.13895\n4      No   529.250605  35704.49394\n5      No   785.655883  38463.49588"},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"outputs_reference":"s3:deepnote-cell-outputs-production/915b66e7-48a5-4e4a-8dc8-6853578e375f"},{"cell_type":"markdown","metadata":{"id":"izo4A3SSvZ7t","cell_id":"edc10777585d4d8dbeab702924c79de5","deepnote_cell_type":"markdown"},"source":"We will start by doing explanatory data analysis (EDA) to examine the data set itself as usual, to get more insight about the data set. ","block_group":"4e027cb982bf477fb142c01800856ac0"},{"cell_type":"markdown","metadata":{"id":"50AbKP76vZ7u","cell_id":"5ca68beea06a45b59a926ee3c105686b","deepnote_cell_type":"markdown"},"source":"---\n\n### 🚩 Exercise 1 (CORE)\n\nExamine the structure of the data. Answer the following questions simply;\n\n1. Do you observe anything from descriptive statistics that may influence our models?\n2.  Are there any missing values in your data ? \n3. Look at how the varibles relate to each other (Response: Default, Predictors: Balance and Income) \n\n<details><summary><b><u>Hint</b></u></summary>\n    \nMaybe some scatterplot and boxplot for each feature in terms of different response class will be useful\n    \n</details>","block_group":"00005-da6d5f4b-bf6b-43f7-9751-a015e6a9924e"},{"cell_type":"code","metadata":{"id":"-mG-rvtrvZ7u","output_cleared":true,"cell_id":"2063b8aba95e445b9843065c9afce8f5","deepnote_cell_type":"code"},"source":"","block_group":"00006-4d9907ea-c7c7-4c16-aee7-5e12c3e00cc1","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"01HS3aS8vZ7u","cell_id":"a1ae883ad0034415847d9725eb17aa73","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your text solution here !!!**\n","block_group":"00007-a3d1594c-000b-48dd-b347-dfa5410fe0a7"},{"cell_type":"markdown","metadata":{"id":"O6TMeCFgAJVt","cell_id":"1921a6489f45432a8ce4cb9d704e94de","deepnote_cell_type":"markdown"},"source":"---\n\n### 🚩 Exercise 2 (CORE)\n\n1. Lets now create our feature matrix and response varible.\n2.  Split the data into training and test sets (**Is there anything you should try account for when splitting the data ?**) Use the test size as $10\\%$ of the whole sample\n\n>- For data splitting consider the the additional argument inside of `train_test_split` function, stratify in terms of the response data set.\n>- You can recall the details from here: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html\n\n3. Convert your response variable into the numerical format\n\n<details><summary><b><u>Hint</b></u></summary>\n    \nYou can consider `LabelEncoder` module for that purpose: https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.LabelEncoder.html\n    \n</details>","block_group":"ea0be26631974fe18baf8658e9152e41"},{"cell_type":"markdown","metadata":{"id":"01HS3aS8vZ7u","cell_id":"a86f8bdf410549e2b967cf7e6bdd4bdd","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your text solution here !!!**\n","block_group":"00007-a3d1594c-000b-48dd-b347-dfa5410fe0a7"},{"cell_type":"markdown","metadata":{"id":"_YZt5su8C7WZ","cell_id":"c289deb9f0464376bc410dac07fb74e8","deepnote_cell_type":"markdown"},"source":"# 2. Logistic Regression <a id='RBH'></a>\n\nAs we recall from our notes, for a binary output $y \\in \\lbrace 0, 1 \\rbrace$, **logistic regression** is a classifier that can be seen a simple generalization of linear regression by making two changes. \n\n- First, we replaced the **Gaussian** distribution of the output $y$ with a **Bernoulli** distribution. \n- Second, we pass the linear function of the inputs, $\\mathbf{w}^T\\mathbf{x}$, through a **link function** $g: \\mathbb{R} \\rightarrow [0,1]$. That is, we assume $y \\sim Bern( g(\\mathbf{w}^T\\mathbf{x}))$\n\nThe link function takes values in the unit interval to ensure that the conditional probability of a success, \n\n$$p(y =1 \\mid \\mathbf{x}) = E[y | \\mathbf{x}] = g(\\mathbf{w}^T\\mathbf{x})$$\n\nis between zero and one. Specifically, in logistic regression, we select the **logistic** link function (sigmoid as an S-shaped), defined as \n\n$$g(\\mathbf{w}^T\\mathbf{x}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T\\mathbf{x})}$$\n\nPutting these two steps together, the logistic regression model is:\n\n$$y \\sim Bern\\left( \\left[1 + \\exp(-\\mathbf{w}^T\\mathbf{x})\\right]^{-1} \\right)$$","block_group":"5d4c4585fb22470f899b46ca6041cb3c"},{"cell_type":"markdown","metadata":{"cell_id":"6d9751b74e30479f92fc2b5c232b9ccc","deepnote_cell_type":"markdown"},"source":"To apply this idea with the help of sklearn, the main function that we need to use is `LogisticRegression`. \n\n- There are important details regarding the implementation of it but the most important note to keep in mind is that the regularization is applied by default with the first input argument `penalty='l2'`\n- For further reading, please refer to this website, https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html","block_group":"3b35cc98caa6439f9945f629b056e77b"},{"cell_type":"markdown","metadata":{"id":"mvrOq4afvZ7z","cell_id":"ee661b8b5ceb4560a506d91097f11ee5","deepnote_cell_type":"markdown"},"source":"### 🚩 Exercise 3 (CORE)\n\n- Fit a binary logistic regression model to the training data by using both balance and income as features, **without any regularization**. \n- State the the accuracy score of the model over the testing data\n\n<details><summary><b><u>Hint</b></u></summary>\n    \nYou can consider the `score` function directly\n    \n</details>\n","block_group":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087"},{"cell_type":"code","metadata":{"id":"i2qXwFXvQbBq","cell_id":"1b37a9019b2a458d82418435ad2f68f3","deepnote_cell_type":"code"},"source":"","block_group":"06b67b2621a543f59f9315b1bf514e90","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"taUDXWD5C3mw","cell_id":"f21e8d7dca0a47448467e889df7ddfac","deepnote_cell_type":"markdown"},"source":"## 2.1 Model Predictions\n\nThe fitted `LogisticRegression` objects provide multiple prediction methods. Specifically: \n\n- `predict` which predicts the class label (either 0 or 1), \n- `predict_proba` which predicts the class probabilities, and \n- `predict_log_proba` which predicts the log probabilities of each class\n","block_group":"3a79551335e44a1aaa9435c11e4d83d0"},{"cell_type":"markdown","metadata":{"id":"ZecdeX9yQdVh","cell_id":"cccc3598a650436cb8f28566d1025485","deepnote_cell_type":"markdown"},"source":"---\n\n### 🚩 Exercise 4 (CORE)\n\nBased on the above fitted logistic regression model, calculate\n\n1. the predictions of the class probabilities\n2. Calculate the mean squared error, accuracy score, AUC score of your logistic regression model's predictions. The function `mean_squared_error` , `accuracy_score` and `roc_auc_score` from `sklearn.metrics` will be useful for this. \n\nSee the function documentation if you need [here](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics).","block_group":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087"},{"cell_type":"code","metadata":{"id":"3Mxs7ORhvZ71","output_cleared":true,"cell_id":"30a9683a1bca44d69d8d0815ae9c75d6","deepnote_cell_type":"code"},"source":"","block_group":"00023-04722d32-f420-4104-b25b-37656a71df76","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"406757a56ce74b65a95e975046a06e6c","deepnote_cell_type":"markdown"},"source":"🏁 Now, is a good point to switch driver and navigator","block_group":"b27e2aaaa1be4a8bb3ff157375458476"},{"cell_type":"markdown","metadata":{"id":"z4NLHMUEdKxP","cell_id":"01489d0be0114c29b4807283f3a68d3f","deepnote_cell_type":"markdown"},"source":"---\n\n### 🚩 Exercise 5 (CORE)\n\nConsider the given pipeline for the logistic regression model simply. \n\n- Run 5-fold Cross Validation for this calculation and use the helper function `tidy_scores()` created above for average score calculation.","block_group":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087"},{"cell_type":"code","metadata":{"id":"9vTpIe932BFz","cell_id":"ee79f11feb8342bb8281c90a46731c87","deepnote_cell_type":"code"},"source":"from sklearn.model_selection import cross_validate\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import Pipeline  # combining classifier steps\n\n# One can add more model in the dictionary\nmodel_dict = {\"log\": LogisticRegression(random_state = 42, penalty = \"none\")}\n\n#print(model_dict[model_name]) \n\nfor model_name in model_dict:\n    logistic_pipe = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"model\", model_dict[model_name])])\n    \n\nlogistic_pipe","block_group":"5c858a7d2d474660bbd690f5dd9255aa","execution_count":null,"outputs":[{"data":{"text/html":"<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                (&#x27;model&#x27;, LogisticRegression(penalty=&#x27;none&#x27;, random_state=42))])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;scaler&#x27;, StandardScaler()),\n                (&#x27;model&#x27;, LogisticRegression(penalty=&#x27;none&#x27;, random_state=42))])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StandardScaler</label><div class=\"sk-toggleable__content\"><pre>StandardScaler()</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(penalty=&#x27;none&#x27;, random_state=42)</pre></div></div></div></div></div></div></div>","text/plain":"Pipeline(steps=[('scaler', StandardScaler()),\n                ('model', LogisticRegression(penalty='none', random_state=42))])"},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"outputs_reference":"s3:deepnote-cell-outputs-production/73367084-a432-4f7f-94bd-ea45f792141d"},{"cell_type":"markdown","metadata":{"id":"2uoEYNyw0Jfm","cell_id":"b74b478a830d434b9839153f96bab722","deepnote_cell_type":"markdown"},"source":"## 2.2 Performance Metrics\n\nA binary classifier can make two types of errors:\n\n- Incorrectly assign an individual __who defaults__ to the __no default__ category. (FN)\n- Incorrectly assign an individual who __does not default__ to the __default__ category. (FP)\n\nwhere Defaulted (coded as 1) and not defaulted (coded as 0). As we can recall from our notes once again, the main quantities are; \n\n\n$$\\text{TP} = \\sum_{n=1}^N \\mathbb{I}(y_n=1)\\mathbb{I}(\\widehat{y}_n=1),\\quad \\text{FP} = \\sum_{n=1}^N \\mathbb{I}(y_n=0)\\mathbb{I}(\\widehat{y}_n=1)$$\n$$\\text{FN} = \\sum_{n=1}^N \\mathbb{I}(y_n=1)\\mathbb{I}(\\widehat{y}_n=0), \\quad \\text{TN} = \\sum_{n=1}^N \\mathbb{I}(y_n=0)\\mathbb{I}(\\widehat{y}_n=0)$$\n\n\nwhere $y_n$ is the true class and $\\widehat{y}_n$ is the estimated class. While the overall error rate is low, the error rate among individuals who defaulted is very high. \n\n- From the perspective of a credit card company that is trying to identify high-risk individuals, this error rate among individuals who default may well be unacceptable.\n- From a different persopective, the **lower FN** values are more important compared to the **lower FP** values in this case","block_group":"37572443777e479c961c872f36c07ffd"},{"cell_type":"markdown","metadata":{"id":"Szq6fbGf05A9","cell_id":"52dd00156f784479823e90363b16c02b","deepnote_cell_type":"markdown"},"source":"---\n\n### 🚩 Exercise 6 (CORE)\n\n- Derive the confusion matrix for the logistic regression model with two predictors by following the above pipeline. Afterwards, derive the followings;\n\n1. False Positive Rate (FPR)\n2. True Positive Rate (Recall)\n3. Precision\n4. F1-score\n\nwithout using any additional built-in function from any module (You need to recall the definitions from our notes) \n\n$$\n\\text{Accuracy} = \\frac{\\text{TP + TN}}{\\text{TP + TN + FP + FN}}\n$$\n\n$$\n\\text{FPR} = \\frac{\\text{FP}}{\\text{FP}+ \\text{TN}}, \\hspace{0.5cm} \\text{Recall (TPR)} = \\frac{\\text{TP}}{\\text{TP}+ \\text{FN}} \\hspace{0.5cm}\n\\text{Precision} = \\frac{\\text{TP}}{\\text{TP}+ \\text{FP}}\n$$\n\n\n$$\n\\text{F1-Score} = 2\\left(\\frac{Precision \\times Recall}{Precision + Recall}\\right) = \\frac{\\text{2TP}}{\\text{2TP + FP + FN}}\n$$\n\n- What can you say about the model performance in terms of Confusion Matrix and obtained quantities ","block_group":"f6b66cac91d847f785cfba2336f4d498"},{"cell_type":"code","metadata":{"id":"yRdjYEgP1ahd","cell_id":"481d971dee5e4e28b64977d63152427e","deepnote_cell_type":"code"},"source":"log_pipe = Pipeline([\n        (\"scaler\", StandardScaler()),\n        (\"model\", LogisticRegression(random_state = 42, penalty = \"none\"))])\n\nlog_pipe.fit(X_train, y_train)\n\n# use the classifier to predict the based on testing data    \n\n\n# get the confusion matrix as a numpy array\n","block_group":"fbd1823ea2af4aa9ac812111011634fe","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"id":"UQrDeC8O8-Nf","cell_id":"4e26bc0661b544b6b957547882b1559e","deepnote_cell_type":"code"},"source":"","block_group":"729debdd4c6f4a4f8ff7ed59e10eda8a","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"Ruvl0mkhF8xL","cell_id":"4697423de9e946f79decee45c7b553ba","deepnote_cell_type":"markdown"},"source":"**!!! Add your comments about the model performance here !!!**\n","block_group":"c14562a79b1c4c2484eae2d114943ddc"},{"cell_type":"markdown","metadata":{"id":"tSdhFuuUjecy","hideCode":false,"hidePrompt":false,"cell_id":"dd611a5c6481444c9e1a3fe1977d1536","deepnote_cell_type":"markdown"},"source":"**F1-score**\n\n- As we have seen above, F1-score is a combination of Recall and Precision. \n- It is typically used when there is an __uneven class distribution__ due to a large number of True Negatives that you are not as focused on. \n\n__Notes__\n\n- Using `sklearn.metrics` this can be gained using `f1_score(y_true=y_test, y_pred=predictions)`","block_group":"c8bf366806bb4d4a92db43e1cf01f2fa"},{"cell_type":"markdown","metadata":{"id":"K7foBrtx5DXO","cell_id":"9ece3d976b7444a5970c7033b86353bd","deepnote_cell_type":"markdown"},"source":"---\n\n### 🚩 Exercise 7 (CORE)\n\n1. Calculate the $F1$ score using `f1_score` function for the logistic regression model with two predictors and compare your result with your calculation obtained before !\n2. Draw ROC curve for the same model. Explain the obtained graphical result.  ","block_group":"0dfc00f147dd429bbe429dc9eda2383d"},{"cell_type":"code","metadata":{"id":"ClWZ7isN5ekk","cell_id":"07cccdb6dc784daab9fcec50fca80e75","deepnote_cell_type":"code"},"source":"","block_group":"0e3f8b2d39af4628a5c2cf013f30875e","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"ZqZpAT1MxolL","cell_id":"bc5eee66adec4d6ebddad98a160f0d73","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your comments here !!!**\n","block_group":"cd451053926d464e8cb9d8b6897c44d1"},{"cell_type":"markdown","metadata":{"id":"cjuQg3ty7XYG","cell_id":"d628e778f9924938bb33663a2d59db9e","deepnote_cell_type":"markdown"},"source":"---\n\n### 🚩 Exercise 8 (CORE)\n\nNow consider the function `LogisticRegressionCV()` using the training data and both 'balance' and 'income' as features. You can use the cv variable as 5 for the 5-fold cross-validation. \n\n- Fit a Logistic Model with `L2` penalty, with the `LogisticRegressionCV()` and calculate the accuracy for test data. \n\n- Play with the `Cs` input argument as well. Discuss how the chosen regularization strength impacts the model's performance.\n\n- Compare your results with the use of `LogisticRegression()` with the suitable penalty term.","block_group":"ef0786a7cfa944fcae2a87d5131dc21f"},{"cell_type":"code","metadata":{"cell_id":"50900475252b419e8fb7be269088f6b2","deepnote_cell_type":"code"},"source":"# Your code here","block_group":"d3bc51fb56dc4fbda71fcd8ac635d564","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"496dc269f5034f2ea5ae0d7d3e1034d3","deepnote_cell_type":"markdown"},"source":"🏁 Now, is a good point to switch driver and navigator","block_group":"eee55a5a0d8e4e82a8c5f749d77664d9"},{"cell_type":"markdown","metadata":{"id":"CbITg5nFR3Ww","cell_id":"02797f6efcea497081e3d4adadfa3fff","deepnote_cell_type":"markdown"},"source":"## 2.4. Grid Search\n\n- Before we start searching over hyperparameters, its worth noting that some of the folds may not have the same distribution of the classes. \n- This means we could get a validation score that may be a poor estimate of performance (for example we may have a fold with very few positive classes or more than usual). \n- Therefore when doing our gridsearch/randomsearch, we should use a `StratifiedKFold` to ensure the distribution of classes in our folds reflects the distribution in the larger data.\n\nYou can check the details for `RandomizedSearchCV` function from here \n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html","block_group":"1a96b02b93e04b3290fe8a49d5549f8a"},{"cell_type":"code","metadata":{"id":"85ckjDn8jec2","cell_id":"c9007eb28baa438099c2a6b81e6db019","deepnote_cell_type":"code"},"source":"from scipy.stats.distributions import uniform, loguniform\n\nlog_pipe = Pipeline([\n            (\"scaler\", StandardScaler()),\n            (\"model\", LogisticRegression(C = 1, random_state=42))])\n\nC_list = []\npwr = -5\nfor i in range(11):\n    C_list.append(2**pwr)\n    pwr+=2\n    \n# specify parameters and distributions to sample from\nlog_param_dist = {'model__C':loguniform(C_list[0], C_list[-1])}\n\nlog_rs = RandomizedSearchCV(______, \n                            param_distributions = log_param_dist,\n                            n_iter = 60, \n                            scoring = [\"accuracy\", \"f1\",\"recall\",\"precision\"], \n                            cv = StratifiedKFold(n_splits=5),\n                            refit = \"accuracy\", \n                            random_state = 42,\n                            return_train_score = True)\n\nlog_rs.fit(______, ______)","block_group":"4754c66c30f84ae59d7b4c0fad9c8fc5","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"sy2nM4wbrx9E","cell_id":"69355fc614a3491290294b250201918b","deepnote_cell_type":"markdown"},"source":"### 🚩 Exercise 9 (CORE)\n\n1. Convert your `log_rs.cv_results_` into pandas data frame and sort the values in terms \"mean_test_accuracy\" in descending order by using `sort_values` function !!!\n\n2. Look at the first 6 values in this sorted values and make comments on test accuracy !!!","block_group":"9ebe0d5c6b8d4f29b8840e4399c9d33c"},{"cell_type":"code","metadata":{"id":"872OWw_fTDWz","cell_id":"522f28aa48b24119959c7ba765852f05","deepnote_cell_type":"code"},"source":"# About the summary of best 6 models \n# Convert your log_rs.cv_results_ into pandas data frame and sort the values in terms \n# \"mean_test_accuracy\" in descending order by using sort_values !!!\n\n# Look at the first 6 values in this sorted values and make comments on test accuracy !!!\n","block_group":"99e1ddfe14684b55afa9cfd1ba23bdd2","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"cell_id":"bba2e06df7ec4c96868cb2dae46d8609","deepnote_cell_type":"markdown"},"source":"## 2.5. Logistic with Regularization <a id='SKV'></a>\n\n- Note that regularized logistic regression is implemented using the ‘liblinear’ library, ‘newton-cg’, ‘sag’, ‘saga’ and ‘lbfgs’ solvers in scikits-learn. Note that regularization is applied by default as $L_2$.\n\n**Warning The choice of the algorithm depends on the penalty chosen**. \nSupported penalties by solver:\n- ‘lbfgs’ - [‘l2’, None]\n\n- ‘liblinear’ - [‘l1’, ‘l2’]\n\n- ‘newton-cg’ - [‘l2’, None]\n\n- ‘newton-cholesky’ - [‘l2’, None]\n\n- ‘sag’ - [‘l2’, None]\n\n- ‘saga’ - [‘elasticnet’, ‘l1’, ‘l2’, None]\n\n- So, we can play with the parameters of `LogisticRegression` function, in terms of our needs.","block_group":"a64a4bb39b654b1d8708da97e48734a1"},{"cell_type":"markdown","metadata":{"cell_id":"953afbfca35442458307f9e46fe0a7f5","deepnote_cell_type":"markdown"},"source":"### 🚩 Exercise 10 (EXTRA)\n\n1. Rebuild your model described above (`log_rs`) with regularization type `L1` norms \n2. Derive the summary performance for the obtained model and discuss your findings","block_group":"1ce20e56418b42ce83999026906e81c3"},{"cell_type":"code","metadata":{"cell_id":"012741679acb4f39add95a491bc5978b","deepnote_cell_type":"code"},"source":"","block_group":"f13c5bfc79fc42e79528f069ee9d3cb6","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"Pew94RYnTVZh","cell_id":"2ed249b2dfaa4338a02125c1840e1254","deepnote_cell_type":"markdown"},"source":"##  3. Improving Models with Imbalances <a id='Imbal'></a>\n\nThere are a number of methods to address imbalances in a dataset when we refine our models, such as:\n\n1. Considering the suitable performance metric\n2. Weighting the classes in the model during training\n3. Resampling the data.\n\nHerein, the main focus is the suitable resampling to change the distribution of the classes in our training data.","block_group":"d4a4f85a180345bcaaabdc2854f104e8"},{"cell_type":"markdown","metadata":{"id":"plVHhxDvjec4","hideCode":false,"hidePrompt":false,"cell_id":"6851ac2735c7454abadc7259b4fbb957","deepnote_cell_type":"markdown"},"source":"## 3.1. Changing Metric\n\n__Optimising for Accuracy__ \n\nDuring hyperparameter cross-validation, we selected the model with the best __overall accuracy__. \n\n- This gives us a model with the smallest possible total number of misclassified observations, irrespective of which class the errors come from. \n\n- ML algorithms typically optimize a reward or cost function computed as a sum over the training examples, the decision rule is likely going to be biased toward the majority class$^1$.\n\n__Notes__\n\n- _\"In other words, the algorithm implicitly learns a model that optimizes the predictions based on the most abundant class in the dataset, in order to minimize the cost or  maximize the reward during training.\"_<sup>1</sup>.\n\n- You can check the details of `classification_report` function from here \n\nhttps://scikit-learn.org/stable/modules/generated/sklearn.metrics.classification_report.html","block_group":"e52682f02b9a41fa89bc1caf2f3d712a"},{"cell_type":"code","metadata":{"id":"53D2HcEQjec4","cell_id":"2028813ada7a4bb692b62d4627d2b92e","deepnote_cell_type":"code"},"source":"# This is the classification report based on the accuracy metric\nfrom sklearn.metrics import classification_report\n\nprint(pd.DataFrame(classification_report(y_test, \n                                   bal_log_rs.predict(X_test), \n                                   labels = None, \n                                   target_names = list(LE.classes_), \n                                   sample_weight = None, \n                                   output_dict = True)).round(2))","block_group":"c2ea010639084c539e1bc883b614e399","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"LXJNCQ4rjec4","hideCode":false,"hidePrompt":false,"cell_id":"64973cd6459d449ea9b639ce83f58631","deepnote_cell_type":"markdown"},"source":"- Changing the metric for what is defined as the _\"best model\"_ can help us prioritise models that make particular errors.\n\nFor example, a credit card company might particularly wish to avoid incorrectly classifying an individual who will default, whereas incorrectly classifying an individual who will not default, though still to be avoided, is less problematic. \n\n- In this case, __recall__ would therefore be a useful metric to use.\n\nRather than running another cross-validation again, provided that in `scoring` you used a list that contained \"recall\", we can just use our results data to pick the model with the best \"recall\" instead of accuracy.","block_group":"13e26ca4257f4191b2679aef0cae116c"},{"cell_type":"markdown","metadata":{"id":"RaxRXbcF-5EX","cell_id":"37bec48e5eee43f7a0e6393b2192a2f9","deepnote_cell_type":"markdown"},"source":"---\n\n### 🚩 Exercise 11 (CORE)\n\nConsider the following function for the new metric, `recall`, and then derive a similar classification report that you observed above","block_group":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087"},{"cell_type":"code","metadata":{"id":"upqmGHZ_jec4","hideCode":false,"hidePrompt":false,"cell_id":"5029f0e4f5f94e7aba4c073ea8d0a62d","deepnote_cell_type":"code"},"source":"from sklearn.base import clone\n\n# we refit the best accuracy model on all the training data\n# so lets do that for the best other metric models\ndef manual_refit(input_model, X, y, gs, metric, disp_df=[]):\n    output_model = clone(input_model)\n    \n    gs_df = pd.DataFrame(gs.cv_results_).sort_values(\"mean_test_\" + metric, ascending = False)\n    \n    if disp_df:\n        display(gs_df[disp_df].head())\n    \n    params = gs_df[\"params\"].iloc[0]\n    output_model = output_model.set_params(**params)\n    output_model = output_model.fit(X, y)\n    \n    return output_model\n\nrec_model = manual_refit(_______, ______, ______, bal_log_rs, \"recall\")","block_group":"aa7228db6fee4d7983b0acc22a4e2702","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"RaKa2Q6Jjec3","hideCode":false,"hidePrompt":false,"cell_id":"ac8385b676534911bfd44872677393db","deepnote_cell_type":"markdown"},"source":"## 3.2. Weighting the classes\n\nDuring model fitting we can assign a larger penalty to wrong predictions on the minority class.\n\nThe heuristic used for `class_weight=\"balanced\"` in Scikit-Learn is:\n\n$$\n\\frac{n}{Nc \\times \\sum\\limits^n_{i=1}I(y_i \\in S)}\n$$\n\nwhere $n$ are the number of samples, $Nc$ the number of classes, $I$ is an indicator function, and $S$ contains the class elements. Remember that the default value is `class_weight=None`. ","block_group":"cde9359c2e1049369145bf467109c612"},{"cell_type":"markdown","metadata":{"id":"WdZKHECXTmgU","cell_id":"9add1a3d97734c1dabff28723dc76ed1","deepnote_cell_type":"markdown"},"source":"---\n\n### 🚩 Exercise 12 (EXTRA)\n\nConduct a `randomsearch` using the following code chunk, this time also looking at both `C` and `class_weight`. Has this improved performance? \n\n1. When you change your refit with other scoring alternatives rather than using \"accuracy\", what is the impact of it on the result (test recall)\n2. Is there any improvement when `n_splits = 10` ?    ","block_group":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087"},{"cell_type":"code","metadata":{"id":"EjE0dt7Kjec3","colab":{"height":206,"base_uri":"https://localhost:8080/"},"outputId":"aedffa0b-ce8e-4a1f-cc7e-b4d0206a3d89","cell_id":"05b3dbedd34c4893a83ced12668b9718","deepnote_cell_type":"code"},"source":"log_pipe = Pipeline([\n            (\"scaler\", StandardScaler()),\n            (\"model\", LogisticRegression(C = 1, random_state=42))])\n    \n# specify parameters and distributions to sample from\nlog_param_dist = {\n    'model__C':loguniform(C_list[0], C_list[-1]),\n    'model__class_weight': [None, \"balanced\"]\n}\n\nbal_log_rs = RandomizedSearchCV(log_pipe, \n                            param_distributions = log_param_dist,\n                            n_iter = 60, \n                            scoring = [\"accuracy\", \"f1\", \"recall\", \"precision\"], \n                            cv = StratifiedKFold(n_splits = 5),\n                            refit = \"recall\", \n                            random_state = 42,\n                            return_train_score = True)\n\nbal_log_rs.fit(X_train, y_train)\n\n# To convert it as a data frame\nbal_log_rs_df = pd.DataFrame(bal_log_rs.cv_results_)\n\n#This is sorted for accuarcy, when you change your refit argument above, be careful about that\nbal_log_rs_df.sort_values(\"mean_test_recall\", ascending=False)[[\"param_model__class_weight\", \n                                                                  \"param_model__C\", \n                                                                  \"mean_test_accuracy\", \"mean_test_recall\", \n                                                                  \"std_test_accuracy\"]].head()","block_group":"6ce31608283947408c022bd4b34ddd73","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"Zvf9Dact_xwz","cell_id":"b49260d971534c849647c3a2dfffb97f","deepnote_cell_type":"markdown"},"source":"---\n\n### 🚩 Exercise 13 (EXTRA)\n\n- Derive the confusion matrix for each model described above, `rec_model` and `bal_log_rs` using the confusion_matrix function. \n- State the similarity and  differences of the obtained confusion matrices ","block_group":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087"},{"cell_type":"code","metadata":{"cell_id":"72e82987c27d499285f12ed154f16db1","deepnote_cell_type":"code"},"source":"","block_group":"cd230c6f8b184529bc845dce07cb49d3","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"WMdQ_gVdjec5","hideCode":false,"hidePrompt":false,"cell_id":"48bd7bed02c248ceb0105c2b97dc9b32","deepnote_cell_type":"markdown"},"source":"## 3.3. Resampling\n\nWe can change the distribution of the classes in our training data. As we discussed in the notes, there are two main approaches, called as,\n\n- Down-sampling the majority class (Undersampling)\n- Up-sampling the minority class (Oversampling)\n \n\n### Under-Sampling\n\nThe Down-sampling (Undersampling) involves randomly removing observations from the majority class to prevent its signal from dominating the learning algorithm. \n","block_group":"cf3f1d02e6df4e7a86294f16edfc75a5"},{"cell_type":"markdown","metadata":{"id":"qctA7g8tZH_C","cell_id":"6d2e61fbeb6f4a3f90b2913bc7170b70","deepnote_cell_type":"markdown"},"source":"__Notes__\n\n- `RandomUnderSampler` is part of the Imblearn package, which allows for a lot of techniques for working with imballanced data.\n- There is a `resample` method in `scikit-learn` but Imblearn is a bit smoother to work with.\n- If you want to use a sampler in a model pipeline then you can use the pipeline from imblearn. Using a sampler in a pipeline ensure you wont be training and validating your data on a smaller/larger sample than normal and get unrepresentative results!\n\nYou can check some of the details for `RandomUnderSampler` from here \n\nhttps://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html","block_group":"017c3f0dfb80497b97101e1848febe9d"},{"cell_type":"code","metadata":{"cell_id":"e1f296ee9fae4fafa272abfd1b914ba7","deepnote_cell_type":"code"},"source":"# Install the imblearn if necessary \n# !pip install imblearn","block_group":"9914d304a28847ecb90808cd3c25206d","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"id":"Pa8blBDVjec6","hideCode":false,"hidePrompt":false,"cell_id":"7b95c116246d46ffa9c825a4a745511c","deepnote_cell_type":"code"},"source":"from imblearn.pipeline import Pipeline as ImPipeline\nfrom imblearn.under_sampling import RandomUnderSampler\n\n\nlog_pipe = ImPipeline([\n    (\"scaler\", StandardScaler()),\n    (\"sampler\", RandomUnderSampler(random_state=123)),\n    (\"model\", LogisticRegression(random_state=42))])\n    \n# specify parameters and distributions to sample from\nlog_param_dist = {'model__C':loguniform(C_list[0], C_list[-1])}\n\nus_log_rs = RandomizedSearchCV(log_pipe, \n                            param_distributions = log_param_dist,\n                            n_iter = 60, \n                            scoring = [\"accuracy\", \"f1\", \"recall\", \"precision\"], \n                            cv = StratifiedKFold(n_splits=5),\n                            refit = \"recall\", \n                            random_state = 42,\n                            return_train_score = True)\n\nus_log_rs.fit(X_train, y_train)","block_group":"f0fe2cb9e0c143678c9414648a61be39","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"id":"NdzVhFDNjec6","cell_id":"6c61f6763d81483194f9b8e8430914fb","deepnote_cell_type":"code"},"source":"us_log_rs_df = pd.DataFrame(us_log_rs.cv_results_)\nus_log_rs_df.sort_values(\"mean_test_recall\", ascending=False)[[\"param_model__C\", \n                                                               \"mean_test_recall\", \n                                                               \"std_test_recall\"]].head()","block_group":"3fdf693503ef4df9bb4393d290a316e0","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"1xk6k_W-jec6","hideCode":false,"hidePrompt":false,"cell_id":"ca76a02e435b406fa76a984db7186546","deepnote_cell_type":"markdown"},"source":"### Oversampling\nData can be oversampled easily by randomly sampling from minority classes with replacement to duplicate original samples.\n\nYou can check some of the details for `RandomOverSampler` from here \n\nhttps://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.RandomOverSampler.html\n\nNotes:\n\n- Make sure to oversample after splitting the training and validation sets or you may \"bleed\" information into the validation sets of the model when trying to test a model\n- In-other-words, make sure it is in a pipeline! If you understand the previous mechanism, you can write the `RandomOverSampler` case below !!!","block_group":"66e49878000f4f33aacd88fdb9c036ad"},{"cell_type":"markdown","metadata":{"id":"4qQM8VKxAybZ","cell_id":"cfdc1e7375b64053a32b79b21b9d2cf3","deepnote_cell_type":"markdown"},"source":"### 🚩 Exercise 14 (CORE)\n\n- Create a similar pipeline for oversampling strategy for the imbalanced data. It is very similar what you observed above as in `log_pipe`\n\n- After getting the results in a similar way, compare the performance in terms of undersampling or oversampling","block_group":"00022-28a7a9d1-e95b-47b7-a31c-9b247c628087"},{"cell_type":"code","metadata":{"cell_id":"358609ef84ed46309fde5835236690ca","deepnote_cell_type":"code"},"source":"","block_group":"04c30d29b70a42efb38ce8c04a4cf7ae","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"ilhDv3WHvZ73","cell_id":"aa89dac665b348acaa806c8c1cc3bab4","deepnote_cell_type":"markdown"},"source":"---\n# 4 Multi-class Logistic <a id='mclog'></a>","block_group":"00026-021075dd-c202-41db-9f49-2b4de4efd791"},{"cell_type":"markdown","metadata":{"id":"NtUQzMD9vZ73","cell_id":"30e4bbe416794a67a0cd9c306290c972","deepnote_cell_type":"markdown"},"source":"For this part, basically we will use the `iris` data again, already available to use and it has nice properties in terms of class size over three species. \n\n- More clearly, this is an example where the response has **3 classes** but we **do not need to worry about the imbalanced issue**","block_group":"00027-068e26c7-da60-4c7c-8551-5f914b53af75"},{"cell_type":"code","metadata":{"id":"kJqCZmu5uZpH","cell_id":"e116cff1c5b341fcb538a59381c24588","deepnote_cell_type":"code"},"source":"# First load the data here\nfrom sklearn.datasets import load_iris\n\n# Loading data\nraw_data = load_iris()\ndata_desc = raw_data.DESCR\ndata = pd.DataFrame(raw_data.data, columns=raw_data.feature_names)\n\n# Some information on data set\ndata.info()\ndata.describe()","block_group":"80f65f8d830b430db221139c8812a297","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 150 entries, 0 to 149\nData columns (total 4 columns):\n #   Column             Non-Null Count  Dtype  \n---  ------             --------------  -----  \n 0   sepal length (cm)  150 non-null    float64\n 1   sepal width (cm)   150 non-null    float64\n 2   petal length (cm)  150 non-null    float64\n 3   petal width (cm)   150 non-null    float64\ndtypes: float64(4)\nmemory usage: 4.8 KB\n"},{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>count</th>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n      <td>150.000000</td>\n    </tr>\n    <tr>\n      <th>mean</th>\n      <td>5.843333</td>\n      <td>3.057333</td>\n      <td>3.758000</td>\n      <td>1.199333</td>\n    </tr>\n    <tr>\n      <th>std</th>\n      <td>0.828066</td>\n      <td>0.435866</td>\n      <td>1.765298</td>\n      <td>0.762238</td>\n    </tr>\n    <tr>\n      <th>min</th>\n      <td>4.300000</td>\n      <td>2.000000</td>\n      <td>1.000000</td>\n      <td>0.100000</td>\n    </tr>\n    <tr>\n      <th>25%</th>\n      <td>5.100000</td>\n      <td>2.800000</td>\n      <td>1.600000</td>\n      <td>0.300000</td>\n    </tr>\n    <tr>\n      <th>50%</th>\n      <td>5.800000</td>\n      <td>3.000000</td>\n      <td>4.350000</td>\n      <td>1.300000</td>\n    </tr>\n    <tr>\n      <th>75%</th>\n      <td>6.400000</td>\n      <td>3.300000</td>\n      <td>5.100000</td>\n      <td>1.800000</td>\n    </tr>\n    <tr>\n      <th>max</th>\n      <td>7.900000</td>\n      <td>4.400000</td>\n      <td>6.900000</td>\n      <td>2.500000</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\ncount         150.000000        150.000000         150.000000   \nmean            5.843333          3.057333           3.758000   \nstd             0.828066          0.435866           1.765298   \nmin             4.300000          2.000000           1.000000   \n25%             5.100000          2.800000           1.600000   \n50%             5.800000          3.000000           4.350000   \n75%             6.400000          3.300000           5.100000   \nmax             7.900000          4.400000           6.900000   \n\n       petal width (cm)  \ncount        150.000000  \nmean           1.199333  \nstd            0.762238  \nmin            0.100000  \n25%            0.300000  \n50%            1.300000  \n75%            1.800000  \nmax            2.500000  "},"execution_count":27,"metadata":{},"output_type":"execute_result"}],"outputs_reference":"s3:deepnote-cell-outputs-production/c9c3d834-2484-4fdb-870f-0f8044c83714"},{"cell_type":"code","metadata":{"cell_id":"72a06766b0db4c198d183b177dda86c2","deepnote_cell_type":"code"},"source":"data.head()","block_group":"07257243d7664ea5a4dc962f797e8728","execution_count":null,"outputs":[{"data":{"text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>sepal length (cm)</th>\n      <th>sepal width (cm)</th>\n      <th>petal length (cm)</th>\n      <th>petal width (cm)</th>\n      <th>species</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>5.1</td>\n      <td>3.5</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>4.9</td>\n      <td>3.0</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>4.7</td>\n      <td>3.2</td>\n      <td>1.3</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.6</td>\n      <td>3.1</td>\n      <td>1.5</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>3.6</td>\n      <td>1.4</td>\n      <td>0.2</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>","text/plain":"   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0                5.1               3.5                1.4               0.2   \n1                4.9               3.0                1.4               0.2   \n2                4.7               3.2                1.3               0.2   \n3                4.6               3.1                1.5               0.2   \n4                5.0               3.6                1.4               0.2   \n\n   species  \n0        0  \n1        0  \n2        0  \n3        0  \n4        0  "},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"outputs_reference":"s3:deepnote-cell-outputs-production/dfe4c7c9-494d-4214-9104-614742499d74"},{"cell_type":"markdown","metadata":{"id":"kMZfWvLKvZ74","cell_id":"29f520bd99134713bf191daa94d224db","deepnote_cell_type":"markdown"},"source":"Some Visual Inspection on the data set","block_group":"00031-b1d09da5-a8c6-46db-a5ec-617662e344a9"},{"cell_type":"code","metadata":{"id":"Rt0cuv8uvZ74","output_cleared":true,"cell_id":"90e2b04be23b4bc68a99f65dc4b1eefe","deepnote_cell_type":"code"},"source":"# Adding the species as a new column\ndata['species'] = raw_data.target\nprint(data.head())","block_group":"00032-d4cd3759-a588-4d95-b34e-507416ebc046","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"b1kX39bpvRlW","cell_id":"4715c13b7f6d4028ac3b4b0d3c580e25","deepnote_cell_type":"markdown"},"source":"\nSince we will be attempting to predict the species of flower which is a finite list of categorical options (ie. Setosa, Virginica, Versicolor), we need to assign a numerical value for each of the species within dummy variables. Dummy variables are variables that take binary (True=1, False=0) assignments based on every category in a given column. For example, our categories are Setosa, Virginica and Versicolor. We will have a column for each of these species and will assign a 1 or 0 for each row depending on what species they are. So a sample row representing Setosa, will have the Setosa column value of 1 and the Virginica and Versicolor values would be assigned as 0.","block_group":"1ffdfc2c38654e45b6b275bac943a84f"},{"cell_type":"code","metadata":{"id":"VcuBnGPavZ73","colab":{"base_uri":"https://localhost:8080/"},"outputId":"6e1a6cad-acb2-454e-ea14-4e1753f4d69e","output_cleared":true,"cell_id":"8f72f70f4c984a64a3b27a0307c7badc","deepnote_cell_type":"code"},"source":"# Consider species_label function as a helper \n# For iris data example \ndef species_label(theta):\n\tif theta==0:\n\t\treturn raw_data.target_names[0]\n\tif theta==1:\n\t\treturn raw_data.target_names[1]\n\tif theta==2:\n\t\treturn raw_data.target_names[2]","block_group":"00030-d0c72235-001d-4cd4-a50f-302b40d9e4d3","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"code","metadata":{"cell_id":"ae8dee28877b4e9a82b92a11d02b38c2","deepnote_cell_type":"code"},"source":"# data = pd.DataFrame(raw_data.data, columns=raw_data.feature_names)\ndata['species'] = [species_label(theta) for theta in raw_data.target]\n#data['species_id'] = raw_data.target\nprint(data.head())\n\n# Class size information\nprint(data.species.value_counts())","block_group":"277d188f3a0e4353953222d1e04807c3","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n0                5.1               3.5                1.4               0.2   \n1                4.9               3.0                1.4               0.2   \n2                4.7               3.2                1.3               0.2   \n3                4.6               3.1                1.5               0.2   \n4                5.0               3.6                1.4               0.2   \n\n  species  \n0  setosa  \n1  setosa  \n2  setosa  \n3  setosa  \n4  setosa  \nsetosa        50\nversicolor    50\nvirginica     50\nName: species, dtype: int64\n"}],"outputs_reference":"dbtable:cell_outputs/93e1fcd8-e109-44e6-b51a-4ce027b7b4de"},{"cell_type":"code","metadata":{"id":"WEq_asGw5d7M","cell_id":"0e0127b394cd40e196613f02b13fb3ae","deepnote_cell_type":"code"},"source":"#Prepare the data set\n\n#Encoding Species columns (to numerical values)\ndata['species'] = data['species'].astype('category').cat.codes\nprint(data['species'])\n\n#Feature & Target Selection\n# X = feature values, all the columns except the last column\nX = data.iloc[:, :-1]\nprint(X.head())\n\n# y = target values, last column of the data frame\ny = data.iloc[:, -1]\nprint(y.head())","block_group":"dd379faf988f488abca7f7e9e5220238","execution_count":null,"outputs":[{"name":"stdout","output_type":"stream","text":"0      0\n1      0\n2      0\n3      0\n4      0\n      ..\n145    2\n146    2\n147    2\n148    2\n149    2\nName: species, Length: 150, dtype: int8\n   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n0                5.1               3.5                1.4               0.2\n1                4.9               3.0                1.4               0.2\n2                4.7               3.2                1.3               0.2\n3                4.6               3.1                1.5               0.2\n4                5.0               3.6                1.4               0.2\n0    0\n1    0\n2    0\n3    0\n4    0\nName: species, dtype: int8\n"}],"outputs_reference":"dbtable:cell_outputs/dcaa1c22-40da-4a35-af95-f772563c4b19"},{"cell_type":"markdown","metadata":{"id":"eJZMp0E13sbM","cell_id":"01e7503da1244731b2551d279ae291db","deepnote_cell_type":"markdown"},"source":"After partitioning your data set into training and testing with %80-%20 rule,\n\n- Create `One-Vs-Rest Logistic` Regression pipeline and train your model\n- Create `Multinomial Logistic` Regression pipeline and train your model","block_group":"73460dd73f12478cb46f204d621c6760"},{"cell_type":"markdown","metadata":{"id":"9Ow2uUjn-5H9","cell_id":"3283b235af0c4dde95676156c7180f15","deepnote_cell_type":"markdown"},"source":"Under this method/strategy a multi-class classification dataset (in our case IRIS) is split into multiple binary classification chunks/datasets. Then a binary classifier (in our case SVC) is trained on each of the binary classification datasets and a prediction(s) is made using the model that has the most confidence value. The IRIS dataset will be split into individual datasets for each Species versus every other Species. Following are the details:\n\n1. Binary Classification: setosa vs versicolor\n2. Binary Classification: setosa vs virginica\n3. Binary Classification: versicolor vs virginica\n\nSide Not: The **OneVsRestClassifier** class is very easy to use and requires that a classifier that is to be used for binary classification be provided to the **OneVsRestClassifier** as an argument. For this purpose, we need to play around the argument `multi_class`","block_group":"b4edccd354944dda92ecf5cda6b6766e"},{"cell_type":"code","metadata":{"cell_id":"58f0e766e99f4b16805c6a39dcc5dead","deepnote_cell_type":"code"},"source":"","block_group":"0608dd349c32434b8b8eae086fed3cf4","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"O26qY6A_-NvN","cell_id":"961d43982d6a4da3ba446e6987668928","deepnote_cell_type":"markdown"},"source":"### 🚩 Exercise 15 (EXTRA)\n\n- Test the performance of `One-vs-Rest` and `Multinomial` Logistic models\n- Derive the confusion matrix for each model and compare the quantities to conduct a model comparison","block_group":"051332069b7848ff8576f3c20537972f"},{"cell_type":"code","metadata":{"id":"c3mFzKbLCI_J","cell_id":"3d75bf2b013c495db14023f6cea0e1dd","deepnote_cell_type":"code"},"source":"","block_group":"22027dc741ef45ecacd8ce0351afa5da","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","metadata":{"id":"EW10NyWz8vK-","cell_id":"75e66084c1974242a1347b0939219ffc","deepnote_cell_type":"markdown"},"source":"---\n\n**!!! Add your comments on obtained  !!!**\n","block_group":"4080fbe3a0b2463cb8e5dcf0b8fc4acc"},{"cell_type":"markdown","metadata":{"cell_id":"800eec6245f44a8baf5280296f55d484","deepnote_cell_type":"markdown"},"source":"# Competing the Worksheet\n\nAt this point you have hopefully been able to complete all the CORE exercises and attempted the EXTRA ones. Now \nis a good time to check the reproducibility of this document by restarting the notebook's\nkernel and rerunning all cells in order.\n\nBefore generating the PDF, please go to Edit -> Edit Notebook Metadata and change 'Student 1' and 'Student 2' in the **name** attribute to include your name. If you are unable to edit the Notebook Metadata, please add a Markdown cell at the top of the notebook with your name(s).\n\nOnce that is done and you are happy with everything, you can then run the following cell \nto generate your PDF. Once generated, please submit this PDF on Learn page by 16:00 PM on the Friday of the week the workshop was given. ","block_group":"b9ff886d63e5443f811db0da53f436f3"},{"cell_type":"code","metadata":{"cell_id":"d92934c222b44b709b3e635f60c52327","deepnote_cell_type":"code"},"source":"!jupyter nbconvert --to pdf mlp_week06.ipynb ","block_group":"d624719b284d4781b754485270b51fe6","execution_count":null,"outputs":[],"outputs_reference":null},{"cell_type":"markdown","source":"<a style='text-decoration:none;line-height:16px;display:flex;color:#5B5B62;padding:10px;justify-content:end;' href='https://deepnote.com?utm_source=created-in-deepnote-cell&projectId=a2a9ec8d-a343-4210-b36b-f9db26268fc5' target=\"_blank\">\n<img alt='Created in deepnote.com' style='display:inline;max-height:16px;margin:0px;margin-right:7.5px;' src='data:image/svg+xml;base64,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' > </img>\nCreated in <span style='font-weight:600;margin-left:4px;'>Deepnote</span></a>","metadata":{"created_in_deepnote_cell":true,"deepnote_cell_type":"markdown"}}],"nbformat":4,"nbformat_minor":0,"metadata":{"deepnote_notebook_id":"324e0f778a0c4e4fad207ae27e5fdc72","deepnote_execution_queue":[]}}